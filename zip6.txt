1. Create a file big.txt:

hdfs dfs -put big.txt /

localhost:9870

var linesRDD = sc.textFile("hdfs://localhost:54310/big.txt")

linesRDD.collect

var wordsRDD = linesRDD.flatMap(_.split(" "))

wordsRDD.collect

var wordsKvRdd = wordsRDD.map((_, 1))

wordsKvRdd.collect

var wordCounts = wordsKvRdd.reduceByKey(_ + _ )

wordCounts.collect

wordCounts.saveAsTextFile("hdfs://localhost:54310/output1")


spark-shell

start-all.sh

jps

