mapper.py

import sys
for line in sys.stdin:
    fields = line.strip().split(",")
    if len(fields) != 3:
        continue
    salary = int(fields[2])
    print("{0}\t{1}".format(salary, 1))


reducer.py

import sys
total_salary = 0
employee_count = 0
for line in sys.stdin:
    salary, count = line.strip().split('\t')
    total_salary += int(salary) * int(count)
    employee_count += int(count)
average_salary = total_salary / employee_count
print("Average salary: {0}".format(average_salary))


input.txt
1,John Doe,50000
2,Jane Smith,60000
3,Bob Johnson,70000
4,Susan Williams,55000
5,Tom Davis,65000
6,Emily Brown,75000
7,Michael Lee,45000
8,Kelly Green,55000
9,David Kim,80000
10,Michelle Wong,90000

$cat input.txt | python3 mapper.py | sort | python3 reducer.py

1. sudo su hduser
2. start-dfs.sh
3. start-yarn.sh
4. jps
5. hdfs dfs -put /input.txt /
6. hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming 3.3.4.jar -files
./mapper.py,./reducer.py -mapper "python3 mapper.py" -reducer "python3 reducer.py" -input
/5000-8.txt -output /0000009
7. Observe the result in localhost:9870
8. Download outputfile and view it for wordcount